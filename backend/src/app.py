"""Python version 3.12"""

import json
import os
import traceback
from typing import Dict, List

import google.generativeai as genai  # type: ignore
import psycopg2
from dotenv import load_dotenv
from voyageai import Client  # type: ignore


class EUAIActQA:
    """
    A class for question-answering based on the EU AI Act
    using embedding retrieval and generative models.

    This class integrates:
    - Embedding retrieval with Voyage AI
    - Answer generation with Google's Gemini API
    - Storage and retrieval of context chunks from PostgreSQL
    - Logging of queries and responses to a separate logs database
    """

    def __init__(self):
        """
        Initializes the QA system by:
        - Loading environment variables
        - Configuring the Voyage and Gemini clients
        """
        load_dotenv()

        # Load environment variables
        self.voyage_client = Client(api_key=os.getenv("VOYAGE_API_KEY"))
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.model = genai.GenerativeModel("gemini-2.0-flash")

    def get_db_connection(self):
        """
        Establishes a connection to the main
        database containing document chunks.

        Returns:
            - psycopg2.connection: A live connection
            - object to the main PostgreSQL database.
        """
        return psycopg2.connect(
            host="db",
            port=os.getenv("DB_PORT"),
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD"),
        )

    def get_logs_db_connection(self):
        """
        Establishes a connection to the logs database
        where queries and answers are stored.

        Returns:
            psycopg2.connection: A live connection object
            to the logs PostgreSQL database.
        """
        return psycopg2.connect(
            host=os.getenv("LOGS_HOST"),
            port=os.getenv("LOGS_DB_PORT"),  # defaults to 5432 internally
            dbname=os.getenv("LOGS_DB"),
            user=os.getenv("LOGS_USER"),
            password=os.getenv("LOGS_PASSWORD"),
        )

    def log_query(self, query: str, relevant_chunks: List[Dict], answer: str):
        """
        Logs the question, answer, and the top
        retrieved document chunks into the logs database.

        Args:
            - query (str): The user's input question.
            - relevant_chunks (List[Dict]): The top-k
            relevant document chunks retrieved from the DB.
            - answer (str): The answer generated by the LLM.

        Notes:
            - Automatically creates the `query_logs` table
            if it doesn't exist.
            - Logs distances and raw JSON for auditability.
        """
        try:
            print("ðŸ“ Logging query to DB...")
            print("LOGS DB HOST:", os.getenv("LOGS_HOST"))
            print("LOGS DB PORT:", os.getenv("LOGS_DB_PORT"))
            conn = self.get_logs_db_connection()
            cur = conn.cursor()

            # Ensure table exists
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS query_logs (
                    id SERIAL PRIMARY KEY,
                    timestamp TIMESTAMP WITH TIME
                    ZONE DEFAULT CURRENT_TIMESTAMP,
                    question TEXT NOT NULL,
                    answer TEXT NOT NULL,
                    top_chunks JSONB,
                    distances FLOAT[]
                )
            """
            )

            # âœ… Proper JSON serialization of chunks
            top_chunks_json = json.dumps(relevant_chunks)

            # âœ… Prepare distances as an array
            distances = [chunk["distance"] for chunk in relevant_chunks]

            # Insert record
            cur.execute(
                "INSERT INTO query_logs (question, answer, "
                "top_chunks, distances) VALUES (%s, %s, %s, %s)",
                (query, answer, top_chunks_json, distances),
            )

            conn.commit()
            cur.close()
            conn.close()
        # type: ignore
        except Exception as e:  # type: ignore
            print("âš ï¸ Failed to log query:", e)
            print(traceback.format_exc())

    def get_relevant_chunks(self, query: str, k: int = 3) -> List[Dict]:
        """
        Retrieves the most relevant document chunks
        from the main database using vector similarity.

        Args:
            - query (str): The user's question to embed
            and search against.
            - k (int): The number of top chunks to retrieve.
            Defaults to 3.

        Returns:
            List[Dict]: A list of dictionaries with keys:
            'text', 'metadata', and 'distance'.
        """
        query_embedding = self.voyage_client.embed(
            [query], model="voyage-2"
        ).embeddings[0]

        conn = self.get_db_connection()
        cur = conn.cursor()

        cur.execute(
            """
            SELECT chunk_text, metadata, embedding <-> %s::vector as distance
            FROM document_chunks
            ORDER BY distance
            LIMIT %s
        """,
            (query_embedding, k),
        )

        results = [
            {"text": row[0], "metadata": row[1], "distance": row[2]}
            for row in cur.fetchall()
        ]

        cur.close()
        conn.close()
        return results

    def generate_answer(self, query: str, relevant_chunks: List[Dict]) -> str:
        """
        Generates an answer to the user's question using
        a Gemini generative model, based on retrieved context.

        Args:
            query (str): The user's input question.
            relevant_chunks (List[Dict]): The retrieved
            context chunks used to generate the answer.

        Returns:
            str: The final answer generated by the LLM.
        """
        context = "\n\n".join([chunk["text"] for chunk in relevant_chunks])
        prompt = f"""Based on the following context from
        the EU AI Act, please answer the question.
        If the answer cannot be found in the context, say so.

        Context:
        {context}

        Question: {query}

        Answer:"""

        response = self.model.generate_content(prompt)
        return response.text


if __name__ == "__main__":
    qa = EUAIActQA()
    chunks = [
        {"text": "Example text 1", "metadata": {}, "distance": 0.1},
        {"text": "Example text 2", "metadata": {}, "distance": 0.2},
        {"text": "Example text 3", "metadata": {}, "distance": 0.3},
    ]
    qa.log_query("What is the AI Act?", chunks, "It " "regulates AI use in the EU.")
